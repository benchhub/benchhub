# v0.2.0 Runtime

## TODO

- [ ] split into small features that can work in parallel w/ v0.1.0
- [ ] job, 
- [ ] resource counting, runtime reports back to core?

## Overview

Provide a managed runtime so user only need to provide code.

## Related

- Previous: [v0.1.0 Micro](../v0.1.0-micro)

## Motivation

Allow running test and benchmarks locally and in the cloud. Running locally is useful for interactive development.
Running in the cloud is for CI and large scale benchmarks. The cloud version should also focus on multi-tenancy and efficient scheduling.

## Specs

There are two scenarios for running benchmarks, local or remote.

Local

- single user
- triggered manually
- small test/benchmark, won't eat all the resources of a mbp
- take a snapshot of local code, so it's possible to compare different branches or even same commit but different parameters

Remote

- multiple users from multiple organizations
- triggered by webhook like git commit, bot command
- small, big, requires multiple nodes and may contain workflow (pipeline, trigger a large test if small test pasts)
- binpack and pick the cheapest offer to reduce cost on cloud service providers

Proposed examples are:

Local Examples

- sort
- strstr
- loc

Remote Examples

- xephon-b
- tpc
  - can use pingcap/go-tpc
  
## Background

In the very first benchhub implementation, I tried to write both benchhub and container orchestration (e.g. k8s) at same time.
The easier approach would be use existing platforms, e.g. CRD in k8s.

Using k8s has the following benefits:

- faster prototype
  - the eco system is large and provide metric, log etc. out of box
- deploy to different cloud provider or on premise cloud is easier because they all talk in YAML

Using k8s has the following drawbacks:

- waste resource, k8s's own scheduler and autoscaler maybe not be that good for benchmark type of batch jobs
- overhead and needless complication due to using container (and container network, storage etc.)

Using VM has the following benefits:

- more knobs to tune?
  - might able to do the same using container ...?
- can use bare metal on providers that provide them
  - this is good for database because you don't binpack databases in performance test
  - it should also be possible for k8s by using label and customize the cloud controller logic?
- for a single vm with a single database, it might be faster to download binary compared w/ pull container, setup container network etc.

Using VM has the following drawbacks:

- deal with different cloud provider API and SDKs
  - should either use interface or grpc to write plugin (latter is similar to terraform) 
  
## Features

### v0.1.1 job

Description

Job is generated by framework, e.g. `gotest` generates a job than build a project and run (specific) tests.

A job describes how to build and run a batch job. There are many types of jobs.
Build, Test, Benchmark, Multi Jobs Container. There can be grouping and dependency between jobs.
In the end it becomes a workflow, but we should focus on simple job first and extend/rewrite later.

A job requires some artifact (e.g. code, docker image) and produce some artifact (e.g. binary, docker image, test result).
Output of one job can become input of another job. We may prefer chain multiple jobs into a bigger one even for local execution.
e.g. instead of having a `travis.yml` with `install`, `script` we can have two jobs, one build binary, one run it.
It's up to the runtime and framework to figure out how to take shortcut and avoid copying context around.
At first this seems overkill, but knowing what a job consumes and produces is better than relying on shared storage and magic paths. 

BenchHub itself does not run the job directly, it queues the job and pick some runtime to run it.
BenchHub tells runtime information it should pass to framework, so the framework knows where and how to upload result.
It is possible to have many to many mapping between scheduler and runtime, but that does not give much benefits.
You can slice a huge runtime to multiple smaller ones for runtime that runs on the cloud (vm or k8s).
So we stick with one scheduler and many runtimes and don't allow a runtime to work for multiple schedulers.

Components

- `bhpb/parameter`
  - parameter (like cli arg, env, system config) are important for both configuration, correctness and performance
    - saving all parameters in a structured manner helps to debug and performance tuning in the long run 
- `bhpb/job`
  - define proto for job, the should be generated by bh command when user run `bh test`, `bh bench`
  - figure out how to define code, container as artifacts
- `core/job`
  - allow user to submit job to scheduler and check job status
  - send the job to scheduler (normally in same process, but we can still use gRPC to allow out of process scheduler)
- `core/scheduler`
  - dispatch job to a runtime based on job requirement and status of runtime(s)
- `core/runtime`
  - define the interface for runtime to implement
  - runtime should be able to do at least one of the following
    - build the code (container)
    - run the code (container)
- `framework/go`
  - generates job from `gotest` and `gobench` config

### v0.1.2 local simple batch

Description

Run a batch job locally using docker? (It might be the easies way to package code ...)
Collect metrics, result, log and report to local benchhub instance.
Each framework can run as a host process and use docker log API and bind mount to access output and data.

It is similar to exp/qaq16, though we may want to use docker API instead of shell out.
Well ... honestly I am not a big fan of docker API client and there were several attempts to write a new one in go.ice.

Components

- `core/runtime`
- `runtimes/local`
  - the control plane as a dedicated process talks to scheduler in gRPC
  - run user specified command directly (quick and dirty and frequently used for local development)
    - still capture log and send analysis result
  - run as a container (if the code will change after submission)
    - package code to a container
    - run the container
    - call framework to analysis output
- `lib/fastmonitor`
  - fetch container metrics using cgroup w/o using the slow docker API

### v0.1.3 k8s simple batch

Description

Run a batch job using a single pod on k8s. Collect metrics, result, log and report to benchhub.
Each framework should have its own docker image that pulls the code, run test/benchmark, collect the output.
The draw back it runs multiple processes in one container and if the test code OOM, the result is gone.
Streaming the result or run the framework as a sidecar container might help.

We also need to consider where to build the container. For language specific benchmark, we can prebuild the images,
and build the code inside those containers. For database benchmark, might need a builder to build database binary,
and another runner image that contains the binary. Though it's possible to build and run database in same image.

Components

- `runtimes/k8s`
  - the control plane as a k8s CRD (it's possible to run it in process w/ proper cert)
- `runtimes/k8s/eks`
- `runtimes/k8s/gke`
- `lib/gogocloud/aws`
  - handy wrapper for aws sdk
- `lib/gogocloud/gcp`
  - handy wrapper for gcp sdk

### v0.1.4 vm simple batch

Description

Allocate a new vm for each job. Both framework and test run in the VM as container or process?
This is a waste of resource for sure, but I want to delay scheduler until more complex workflow is introduced.

Components

- `runtimes/remote`
- `runtimes/remote/aws`
- `runtimes/remote/ali`
- `runtimes/remote/gcp`
